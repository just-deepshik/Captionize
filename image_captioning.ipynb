{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSADNqfG8qZd","executionInfo":{"status":"ok","timestamp":1755666789063,"user_tz":-330,"elapsed":23238,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"10c786ff-29f8-48e4-93f6-7a56b01cfbe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":[],"metadata":{"id":"tdAQdqA1jUVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","import os\n","import pickle\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Concatenate, Layer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Import necessary libraries for BLEU score\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n","\n","# USER PATHS\n","ZIP_PATH = \"/content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/images\"           # Folder with images (not zipped)\n","CAPTION_FILE = \"/content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/captions.txt\"  # Captions file\n","FEATURES_FILE = \"/content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/features.pkl\"\n","TOKENIZER_FILE = \"/content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/tokenizer.pkl\"\n","MODEL_PATH = \"/content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\"\n","\n","# 1. Extract or Load Features\n","\n","def extract_image_id(filename):\n","    return os.path.splitext(os.path.basename(filename))[0]\n","\n","if not os.path.exists(FEATURES_FILE):\n","    print(\"Extracting features from images folder...\")\n","    base_model = InceptionV3(weights='imagenet')\n","    model_incep = Model(inputs=base_model.input, outputs=base_incep.layers[-2].output)\n","\n","    features = {}\n","    image_files = [f for f in os.listdir(ZIP_PATH) if f.lower().endswith('.jpg')]\n","    print(f\"Total images found: {len(image_files)}\")\n","    for file_name in tqdm(image_files):\n","        img_path = os.path.join(ZIP_PATH, file_name)\n","        img = Image.open(img_path).convert('RGB')\n","        img = img.resize((299, 299))\n","        image = img_to_array(img)\n","        image = np.expand_dims(image, axis=0)\n","        image = preprocess_input(image)\n","        feature = model_incep.predict(image, verbose=0)\n","        img_id = extract_image_id(file_name)\n","        features[img_id] = feature\n","    with open(FEATURES_FILE, 'wb') as f:\n","        pickle.dump(features, f)\n","    print(f\"Saved features for {len(features)} images.\")\n","else:\n","    print(f\"Loading existing features from {FEATURES_FILE}...\")\n","    with open(FEATURES_FILE, 'rb') as f:\n","        features = pickle.load(f)\n","    print(f\"Loaded features for {len(features)} images.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYDEuPVa80R_","executionInfo":{"status":"ok","timestamp":1755666804434,"user_tz":-330,"elapsed":10452,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"c9d11d9a-56cb-4b0c-dca8-6276f19c393d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading existing features from /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/features.pkl...\n","Loaded features for 8091 images.\n"]}]},{"cell_type":"code","source":["#  2. Load and Process Captions\n","\n","mapping = {}\n","print(f\"Reading captions from: {CAPTION_FILE}\")\n","with open(CAPTION_FILE, 'r') as f:\n","    captions_doc = f.read()\n","\n","lines = captions_doc.strip().split('\\n')\n","print(f\"Total lines in captions file: {len(lines)}\")\n","\n","for line in lines:\n","    if len(line) < 2 or ',' not in line:\n","        continue\n","    image_id, caption = line.split(',', 1)\n","    image_id = image_id.strip()\n","    caption = caption.strip()\n","    image_id = image_id.split('.')[0]\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    mapping[image_id].append(caption)\n","\n","print(f\"Unique image IDs in captions: {len(mapping)}\")\n","\n","def clean_captions(mapping):\n","    for key, caps in mapping.items():\n","        for i in range(len(caps)):\n","            caption = caps[i].lower()\n","            caption = re.sub(r'[^a-z ]', '', caption)\n","            caption = re.sub(r'\\s+', ' ', caption).strip()\n","            caps[i] = 'startseq ' + ' '.join([w for w in caption.split() if len(w) > 1]) + ' endseq'\n","\n","clean_captions(mapping)\n","print(\"Captions cleaned.\")\n","\n","caption_ids = set(mapping.keys())\n","feature_ids = set(features.keys())\n","\n","print(f\"Number of caption image IDs: {len(caption_ids)}\")\n","print(f\"Number of feature image IDs: {len(feature_ids)}\")\n","\n","# Add print statements to help diagnose the mismatch\n","print(\"\\nFirst 10 Feature Image IDs:\", list(feature_ids)[:10])\n","print(\"First 10 Caption Image IDs:\", list(caption_ids)[:10])\n","\n","\n","common_ids = caption_ids.intersection(feature_ids)\n","print(f\"Number of common IDs: {len(common_ids)}\")\n","\n","if len(common_ids) == 0:\n","    raise ValueError(\"No common image IDs between captions and features! Check dataset and filenames.\")\n","\n","filtered_mapping = {img_id: mapping[img_id] for img_id in common_ids}\n","image_ids = list(filtered_mapping.keys())\n","\n","all_captions = [cap for caps in filtered_mapping.values() for cap in caps]\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmmtN6lU-C6v","executionInfo":{"status":"ok","timestamp":1755666834751,"user_tz":-330,"elapsed":1023,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"0413d251-5b72-40be-8573-861cfc600a86"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading captions from: /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/captions.txt\n","Total lines in captions file: 40456\n","Unique image IDs in captions: 8092\n","Captions cleaned.\n","Number of caption image IDs: 8092\n","Number of feature image IDs: 8091\n","\n","First 10 Feature Image IDs: ['473220329_819a913bbb', '864290968_eccb46d5ab', '3429465163_fb8ac7ce7f', '2701603045_6cbdc4ce7c', '3690431163_1d81e19549', '2255332561_3375897ff0', '3106787167_e5f2312622', '309049466_1d7e7d5fc2', '2924870944_90ff9eca1a', '2249264723_d08655d9f2']\n","First 10 Caption Image IDs: ['473220329_819a913bbb', '864290968_eccb46d5ab', '3429465163_fb8ac7ce7f', '2701603045_6cbdc4ce7c', '3690431163_1d81e19549', '2255332561_3375897ff0', '3106787167_e5f2312622', '309049466_1d7e7d5fc2', '2924870944_90ff9eca1a', '2249264723_d08655d9f2']\n","Number of common IDs: 8091\n"]}]},{"cell_type":"code","source":["# 3. Create or Load Tokenizer\n","\n","if not os.path.exists(TOKENIZER_FILE):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(all_captions)\n","    with open(TOKENIZER_FILE, 'wb') as f:\n","        pickle.dump(tokenizer, f)\n","    print(f\"Tokenizer created and saved to {TOKENIZER_FILE}\")\n","else:\n","    with open(TOKENIZER_FILE, 'rb') as f:\n","        tokenizer = pickle.load(f)\n","    print(\"Tokenizer loaded.\")\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","max_length = max(len(c.split()) for c in all_captions)\n","\n","print(f\"Vocabulary size: {vocab_size}\")\n","print(f\"Max caption length: {max_length}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EDp_TiY-PiV","executionInfo":{"status":"ok","timestamp":1755666939054,"user_tz":-330,"elapsed":45,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"4d45d035-cf70-4366-f194-974fc4215090"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer loaded.\n","Vocabulary size: 8768\n","Max caption length: 34\n"]}]},{"cell_type":"code","source":["#  4. Train/Validation Split\n","\n","train_ids, val_ids = train_test_split(image_ids, test_size=0.1, random_state=42)\n","print(f\"Training samples: {len(train_ids)}, Validation samples: {len(val_ids)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nX_eNyY2-WOZ","executionInfo":{"status":"ok","timestamp":1755666947487,"user_tz":-330,"elapsed":22,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"364aca75-92c2-4b06-f55e-3f0d50078284"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 7281, Validation samples: 810\n"]}]},{"cell_type":"code","source":["# 5. Data Generator\n","\n","batch_size = 64\n","\n","def data_generator(image_ids, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","    X1, X2, y = [], [], []\n","    n = 0\n","    while True:\n","        for img_id in image_ids:\n","            caps = mapping[img_id]\n","            for cap in caps:\n","                seq = tokenizer.texts_to_sequences([cap])[0]\n","                for i in range(1, len(seq)):\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\n","                    X1.append(features[img_id][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","                    n += 1\n","\n","                    if n == batch_size:\n","                        yield (np.array(X1), np.array(X2)), np.array(y)\n","                        X1, X2, y = [], [], []\n","                        n = 0"],"metadata":{"id":"jHJOFXac-ayc","executionInfo":{"status":"ok","timestamp":1755666949435,"user_tz":-330,"elapsed":8,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#  6. Bahdanau Attention\n","class BahdanauAttention(Layer):\n","    def __init__(self, units):\n","        super().__init__()\n","        self.W1 = Dense(units)\n","        self.W2 = Dense(units)\n","        self.V = Dense(1)\n","\n","    def call(self, query, values):\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        return context_vector, attention_weights"],"metadata":{"id":"ypV5vTyJ-faw","executionInfo":{"status":"ok","timestamp":1755666951722,"user_tz":-330,"elapsed":9,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#  7. Build the Model\n","\n","units = 256\n","\n","inputs1 = Input(shape=(2048,))\n","fe1 = Dropout(0.4)(inputs1)\n","fe2 = Dense(units, activation='relu')(fe1)\n","\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, units, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)\n","se3 = LSTM(units, return_sequences=True)(se2)\n","\n","attention = BahdanauAttention(units)\n","context_vector, attention_weights = attention(fe2, se3)\n","\n","decoder1 = Concatenate(axis=-1)([fe2, context_vector])\n","decoder2 = Dense(units, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"XidBb2kF-j_d","executionInfo":{"status":"ok","timestamp":1755666958669,"user_tz":-330,"elapsed":2611,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"5477bf7e-c602-499d-f4dc-73629b0f4401"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m2,244,608\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bahdanau_attention  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m131,841\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m1\u001b[0m)]    │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bahdanau_attenti… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8768\u001b[0m)      │  \u001b[38;5;34m2,253,376\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,244,608</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bahdanau_attention  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,841</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]    │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bahdanau_attenti… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8768</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,253,376</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,811,009\u001b[0m (22.17 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,811,009</span> (22.17 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,811,009\u001b[0m (22.17 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,811,009</span> (22.17 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","source":["#  8. Wrap Generators into tf.data.Dataset\n","\n","output_signature = (\n","    (\n","        tf.TensorSpec(shape=(None, 2048), dtype=tf.float32),\n","        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),\n","    ),\n","    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)\n",")\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","    lambda: data_generator(train_ids, filtered_mapping, features, tokenizer, max_length, vocab_size, batch_size),\n","    output_signature=output_signature,\n",").prefetch(tf.data.AUTOTUNE)\n","\n","val_dataset = tf.data.Dataset.from_generator(\n","    lambda: data_generator(val_ids, filtered_mapping, features, tokenizer, max_length, vocab_size, batch_size),\n","    output_signature=output_signature,\n",").prefetch(tf.data.AUTOTUNE)\n","\n","steps_per_epoch = max(len(train_ids) * 5 // batch_size, 1)  # approx 5 captions/image\n","validation_steps = max(len(val_ids) * 5 // batch_size, 1)"],"metadata":{"id":"EZ9MLQgf-oyE","executionInfo":{"status":"ok","timestamp":1755666961448,"user_tz":-330,"elapsed":51,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["checkpoint = ModelCheckpoint(MODEL_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n","\n","print(\"Starting model training...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=20,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_data=val_dataset,\n","    validation_steps=validation_steps,\n","    callbacks=[checkpoint, earlystop]\n",")\n","print(\"Model training finished.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IK-t8XR9-s2q","executionInfo":{"status":"ok","timestamp":1755667228888,"user_tz":-330,"elapsed":264388,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"c2f08c6e-a70b-4640-db25-ff88085ac694"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting model training...\n","Epoch 1/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6.2915\n","Epoch 1: val_loss improved from inf to 5.33909, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - loss: 6.2888 - val_loss: 5.3391\n","Epoch 2/20\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.2125\n","Epoch 2: val_loss improved from 5.33909 to 4.93244, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step - loss: 5.2123 - val_loss: 4.9324\n","Epoch 3/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7978\n","Epoch 3: val_loss improved from 4.93244 to 4.70125, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.7976 - val_loss: 4.7012\n","Epoch 4/20\n","\u001b[1m567/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.5633\n","Epoch 4: val_loss improved from 4.70125 to 4.53338, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 4.5631 - val_loss: 4.5334\n","Epoch 5/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.3800\n","Epoch 5: val_loss improved from 4.53338 to 4.42670, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 4.3797 - val_loss: 4.4267\n","Epoch 6/20\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3352\n","Epoch 6: val_loss improved from 4.42670 to 4.36298, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 4.3352 - val_loss: 4.3630\n","Epoch 7/20\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.1522\n","Epoch 7: val_loss improved from 4.36298 to 4.30534, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 4.1523 - val_loss: 4.3053\n","Epoch 8/20\n","\u001b[1m567/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.2492\n","Epoch 8: val_loss improved from 4.30534 to 4.23047, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 4.2491 - val_loss: 4.2305\n","Epoch 9/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.2026\n","Epoch 9: val_loss improved from 4.23047 to 4.18354, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 4.2025 - val_loss: 4.1835\n","Epoch 10/20\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0592\n","Epoch 10: val_loss did not improve from 4.18354\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 4.0592 - val_loss: 4.2223\n","Epoch 11/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.0086\n","Epoch 11: val_loss improved from 4.18354 to 4.17645, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 4.0082 - val_loss: 4.1765\n","Epoch 12/20\n","\u001b[1m567/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.8399\n","Epoch 12: val_loss did not improve from 4.17645\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 3.8398 - val_loss: 4.1893\n","Epoch 13/20\n","\u001b[1m565/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.7144\n","Epoch 13: val_loss improved from 4.17645 to 4.11873, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 3.7144 - val_loss: 4.1187\n","Epoch 14/20\n","\u001b[1m565/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.7895\n","Epoch 14: val_loss improved from 4.11873 to 4.09006, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 3.7891 - val_loss: 4.0901\n","Epoch 15/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.6412\n","Epoch 15: val_loss improved from 4.09006 to 4.06266, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 3.6411 - val_loss: 4.0627\n","Epoch 16/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6384\n","Epoch 16: val_loss did not improve from 4.06266\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 3.6385 - val_loss: 4.0714\n","Epoch 17/20\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.6370\n","Epoch 17: val_loss did not improve from 4.06266\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 3.6369 - val_loss: 4.0682\n","Epoch 18/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6589\n","Epoch 18: val_loss improved from 4.06266 to 4.04901, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 3.6590 - val_loss: 4.0490\n","Epoch 19/20\n","\u001b[1m567/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6229\n","Epoch 19: val_loss improved from 4.04901 to 4.02630, saving model to /content/drive/MyDrive/Predictive Analysis Lab/Evo Astra/model_9.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 3.6229 - val_loss: 4.0263\n","Epoch 20/20\n","\u001b[1m566/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.5953\n","Epoch 20: val_loss did not improve from 4.02630\n","\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.5954 - val_loss: 4.0696\n","Restoring model weights from the end of the best epoch: 19.\n","Model training finished.\n"]}]},{"cell_type":"code","source":["# 10. Evaluate with BLEU Score (on validation set)\n","\n","print(\"\\nCalculating BLEU score on the validation set...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frCBPCbKXQNy","executionInfo":{"status":"ok","timestamp":1755667267402,"user_tz":-330,"elapsed":44,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"e519da16-52c8-4abf-f7fd-e35a454c5aa8"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Calculating BLEU score on the validation set...\n"]}]},{"cell_type":"code","source":["#  Helper function for BLEU evaluation\n","\n","# Map predicted index back to a word\n","def idx_to_word(integer, tokenizer):\n","    return next((word for word, index in tokenizer.word_index.items() if index == integer), None)\n","\n","# Function to generate caption for BLEU scoring\n","def generate_caption_for_bleu(model, tokenizer, photo, max_length):\n","    in_text = 'startseq'\n","    for _ in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n","        yhat = model.predict([photo, sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = idx_to_word(yhat, tokenizer)\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","    return in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\").split()  # return as list of words\n","\n","#  BLEU score calculation\n","references = []\n","candidates = []\n","\n","for img_id in tqdm(val_ids):\n","    if img_id in features and img_id in filtered_mapping:\n","        # Reference captions\n","        reference_captions = [\n","            cap.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip().split()\n","            for cap in filtered_mapping[img_id]\n","        ]\n","        references.append(reference_captions)\n","\n","        # Candidate caption\n","        candidate_caption = generate_caption_for_bleu(model, tokenizer, features[img_id], max_length)\n","        candidates.append(candidate_caption)\n","    else:\n","        print(f\"Warning: Skipping BLEU calculation for {img_id} due to missing data.\")\n","\n","if references and candidates and len(references) == len(candidates):\n","    print(\"Calculating BLEU scores...\")\n","    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n","    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))\n","    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0))\n","    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","    print(f\"BLEU-1: {bleu1:.4f}\")\n","    print(f\"BLEU-2: {bleu2:.4f}\")\n","    print(f\"BLEU-3: {bleu3:.4f}\")\n","    print(f\"BLEU-4: {bleu4:.4f}\")\n","else:\n","    print(\"Skipping BLEU score calculation: No valid references or candidates found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2RwfRlmXRFS","executionInfo":{"status":"ok","timestamp":1755667850602,"user_tz":-330,"elapsed":579427,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"82da5660-50e5-4d11-ed65-251e63da0209"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 810/810 [09:38<00:00,  1.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Calculating BLEU scores...\n","BLEU-1: 0.4372\n","BLEU-2: 0.2415\n","BLEU-3: 0.1328\n","BLEU-4: 0.0706\n"]}]},{"cell_type":"code","source":["# Generate captions for validation images and collect references\n","for img_id in tqdm(val_ids):\n","    if img_id in features and img_id in filtered_mapping:\n","        # Get reference captions (cleaned and split into words)\n","        reference_captions = [cap.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip().split() for cap in filtered_mapping[img_id]]\n","        references.append(reference_captions)\n","\n","        # Generate candidate caption (split into words)\n","        candidate_caption = generate_caption_for_bleu(model, tokenizer, features[img_id], max_length)\n","        candidates.append(candidate_caption)\n","    else:\n","        print(f\"Warning: Skipping BLEU calculation for {img_id} due to missing data.\")\n","\n","\n","if references and candidates and len(references) == len(candidates):\n","    # Calculate BLEU scores\n","    # weights=(1, 0, 0, 0) for BLEU-1, (0.5, 0.5, 0, 0) for BLEU-2, etc.\n","    print(\"Calculating BLEU scores...\")\n","    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n","    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))\n","    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0))\n","    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","    print(f\"BLEU-1: {bleu1:.4f}\")\n","    print(f\"BLEU-2: {bleu2:.4f}\")\n","    print(f\"BLEU-3: {bleu3:.4f}\")\n","    print(f\"BLEU-4: {bleu4:.4f}\")\n","else:\n","    print(\"Skipping BLEU score calculation: No valid references or candidates found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6H3e5q8VXbTI","executionInfo":{"status":"ok","timestamp":1755668459693,"user_tz":-330,"elapsed":592136,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"c91017a4-7433-4e75-a026-916b8bbce312"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 810/810 [09:50<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Calculating BLEU scores...\n","BLEU-1: 0.4372\n","BLEU-2: 0.2415\n","BLEU-3: 0.1328\n","BLEU-4: 0.0706\n"]}]},{"cell_type":"code","source":["from google.colab import output\n","from PIL import Image\n","from io import BytesIO\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","\n","# Global variable to store uploaded image\n","uploaded_img = None\n","model_incep = None  # will hold InceptionV3 model for feature extraction\n","\n","def idx_to_word(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None\n","\n","def upload_image(file_bytes):\n","    global uploaded_img\n","    uploaded_img = Image.open(BytesIO(bytearray(file_bytes))).convert('RGB')\n","    return \"Image received!\"\n","\n","def generate_caption():\n","    global uploaded_img, model, tokenizer, max_length, model_incep\n","\n","    if uploaded_img is None:\n","        return \"No image uploaded yet!\"\n","\n","    img_resized = uploaded_img.resize((299, 299))\n","    image_array = img_to_array(img_resized)\n","    image_array = np.expand_dims(image_array, axis=0)\n","    image_array = preprocess_input(image_array)\n","\n","    if model_incep is None:\n","        base_model = InceptionV3(weights='imagenet')\n","        model_incep = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n","\n","    image_features = model_incep.predict(image_array, verbose=0)\n","\n","    in_text = 'startseq'\n","    for _ in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n","        yhat = model.predict([image_features, sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = idx_to_word(yhat, tokenizer)\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","\n","    caption = in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n","    return caption\n","\n","output.register_callback('notebook.upload_image', upload_image)\n","output.register_callback('notebook.generate_caption', generate_caption)"],"metadata":{"id":"rvoGQh6RZNUy","executionInfo":{"status":"ok","timestamp":1755668536428,"user_tz":-330,"elapsed":48,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","\n","display(HTML('''\n","<link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&display=swap\" rel=\"stylesheet\">\n","\n","<style>\n","  body {\n","    font-family: 'Poppins', sans-serif;\n","    background: linear-gradient(120deg, #3c8ce7, #00eaff);\n","    background-size: 200% 200%;\n","    animation: auroraBG 8s ease infinite;\n","    color: white;\n","    text-align: center;\n","    padding: 40px;\n","  }\n","\n","  @keyframes auroraBG {\n","    0% {background-position: 0% 50%;}\n","    50% {background-position: 100% 50%;}\n","    100% {background-position: 0% 50%;}\n","  }\n","\n","  h2 {\n","    font-size: 2rem;\n","    margin-bottom: 20px;\n","    background: linear-gradient(90deg, #000080, #000000);\n","    -webkit-background-clip: text;\n","    -webkit-text-fill-color: transparent;\n","    font-weight: 600;\n","  }\n","\n","  .glass-card {\n","    backdrop-filter: blur(15px);\n","    background: rgba(255, 255, 255, 0.15);\n","    border: 1px solid rgba(255, 255, 255, 0.25);\n","    border-radius: 20px;\n","    padding: 25px;\n","    width: 340px;\n","    margin: auto;\n","    box-shadow: 0 8px 32px rgba(0,0,0,0.2);\n","    transition: transform 0.3s ease, box-shadow 0.3s ease;\n","  }\n","  .glass-card:hover {\n","    transform: translateY(-5px);\n","    box-shadow: 0 12px 40px rgba(0,0,0,0.3);\n","  }\n","\n","  .glass-card img {\n","    width: 100%;\n","    border-radius: 15px;\n","    box-shadow: 0 5px 20px rgba(0,0,0,0.2);\n","    margin-bottom: 15px;\n","  }\n","\n","  #fileInput {\n","    margin-top: 10px;\n","    padding: 12px 18px;\n","    font-weight: 500;\n","    font-size: 0.95rem;\n","    border: none;\n","    border-radius: 8px;\n","    background: rgba(255,255,255,0.2);\n","    color: #000000;\n","    cursor: pointer;\n","    transition: background 0.3s ease;\n","  }\n","  #fileInput:hover {\n","    background: rgba(174, 228, 255, 0.5);\n","  }\n","\n","  #generate-btn {\n","    background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","    border: none;\n","    padding: 12px 28px;\n","    font-size: 1rem;\n","    font-weight: 600;\n","    border-radius: 25px;\n","    cursor: pointer;\n","    color: #000000;\n","    box-shadow: 0 5px 20px rgba(60, 140, 231, 0.4);\n","    transition: transform 0.2s ease, box-shadow 0.3s ease;\n","    margin-top: 10px;\n","  }\n","  #generate-btn:disabled {\n","    background: rgba(255,255,255,0.3);\n","    cursor: not-allowed;\n","    box-shadow: none;\n","  }\n","  #generate-btn:hover:not(:disabled) {\n","    transform: translateY(-2px);\n","    box-shadow: 0 8px 25px rgba(60, 140, 231, 0.6);\n","  }\n","\n","  #caption-output {\n","    margin-top: 20px;\n","    font-size: 1.1rem;\n","    font-family: 'Libre Baskerville', serif;\n","    color: #000000;\n","    min-height: 40px;\n","  }\n","</style>\n","\n","<div class=\"glass-card\">\n","  <h2>Automatic Image Caption Generator</h2>\n","  <input type=\"file\" id=\"fileInput\" accept=\"image/*\" />\n","  <img id=\"preview\" src=\"\" style=\"display:none;\" />\n","  <br/>\n","  <button id=\"generate-btn\" disabled>Generate Caption</button>\n","  <div id=\"caption-output\"></div>\n","</div>\n","\n","<script>\n","  const input = document.getElementById('fileInput');\n","  const preview = document.getElementById('preview');\n","  const generateBtn = document.getElementById('generate-btn');\n","  const captionOutput = document.getElementById('caption-output');\n","\n","  input.onchange = evt => {\n","    const [file] = input.files;\n","    if (file) {\n","      preview.src = URL.createObjectURL(file);\n","      preview.style.display = 'block';\n","      generateBtn.disabled = false;\n","      captionOutput.textContent = '';\n","\n","      const reader = new FileReader();\n","      reader.onload = function() {\n","        const arrayBuffer = reader.result;\n","        const bytes = new Uint8Array(arrayBuffer);\n","        google.colab.kernel.invokeFunction('notebook.upload_image', [Array.from(bytes)], {});\n","      };\n","      reader.readAsArrayBuffer(file);\n","    }\n","  };\n","\n","  generateBtn.onclick = () => {\n","    captionOutput.textContent = 'Generating caption...';\n","\n","    google.colab.kernel.invokeFunction('notebook.generate_caption', [], {}).then(result => {\n","      captionOutput.textContent = result.data['text/plain'].replace(/'/g, \"\");\n","    }).catch(() => {\n","      captionOutput.textContent = '⚠️ Error generating caption';\n","    });\n","  };\n","</script>\n","'''))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"js2C9lM3gMdH","executionInfo":{"status":"ok","timestamp":1755097348739,"user_tz":-330,"elapsed":15,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"3d9dfb06-c800-4130-c743-faed28278588"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&display=swap\" rel=\"stylesheet\">\n","\n","<style>\n","  body {\n","    font-family: 'Poppins', sans-serif;\n","    background: linear-gradient(120deg, #3c8ce7, #00eaff);\n","    background-size: 200% 200%;\n","    animation: auroraBG 8s ease infinite;\n","    color: white;\n","    text-align: center;\n","    padding: 40px;\n","  }\n","\n","  @keyframes auroraBG {\n","    0% {background-position: 0% 50%;}\n","    50% {background-position: 100% 50%;}\n","    100% {background-position: 0% 50%;}\n","  }\n","\n","  h2 {\n","    font-size: 2rem;\n","    margin-bottom: 20px;\n","    background: linear-gradient(90deg, #000080, #000000);\n","    -webkit-background-clip: text;\n","    -webkit-text-fill-color: transparent;\n","    font-weight: 600;\n","  }\n","\n","  .glass-card {\n","    backdrop-filter: blur(15px);\n","    background: rgba(255, 255, 255, 0.15);\n","    border: 1px solid rgba(255, 255, 255, 0.25);\n","    border-radius: 20px;\n","    padding: 25px;\n","    width: 340px;\n","    margin: auto;\n","    box-shadow: 0 8px 32px rgba(0,0,0,0.2);\n","    transition: transform 0.3s ease, box-shadow 0.3s ease;\n","  }\n","  .glass-card:hover {\n","    transform: translateY(-5px);\n","    box-shadow: 0 12px 40px rgba(0,0,0,0.3);\n","  }\n","\n","  .glass-card img {\n","    width: 100%;\n","    border-radius: 15px;\n","    box-shadow: 0 5px 20px rgba(0,0,0,0.2);\n","    margin-bottom: 15px;\n","  }\n","\n","  #fileInput {\n","    margin-top: 10px;\n","    padding: 12px 18px;\n","    font-weight: 500;\n","    font-size: 0.95rem;\n","    border: none;\n","    border-radius: 8px;\n","    background: rgba(255,255,255,0.2);\n","    color: #000000;\n","    cursor: pointer;\n","    transition: background 0.3s ease;\n","  }\n","  #fileInput:hover {\n","    background: rgba(174, 228, 255, 0.5);\n","  }\n","\n","  #generate-btn {\n","    background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","    border: none;\n","    padding: 12px 28px;\n","    font-size: 1rem;\n","    font-weight: 600;\n","    border-radius: 25px;\n","    cursor: pointer;\n","    color: #000000;\n","    box-shadow: 0 5px 20px rgba(60, 140, 231, 0.4);\n","    transition: transform 0.2s ease, box-shadow 0.3s ease;\n","    margin-top: 10px;\n","  }\n","  #generate-btn:disabled {\n","    background: rgba(255,255,255,0.3);\n","    cursor: not-allowed;\n","    box-shadow: none;\n","  }\n","  #generate-btn:hover:not(:disabled) {\n","    transform: translateY(-2px);\n","    box-shadow: 0 8px 25px rgba(60, 140, 231, 0.6);\n","  }\n","\n","  #caption-output {\n","    margin-top: 20px;\n","    font-size: 1.1rem;\n","    font-family: 'Libre Baskerville', serif;\n","    color: #000000;\n","    min-height: 40px;\n","  }\n","</style>\n","\n","<div class=\"glass-card\">\n","  <h2>Automatic Image Caption Generator</h2>\n","  <input type=\"file\" id=\"fileInput\" accept=\"image/*\" />\n","  <img id=\"preview\" src=\"\" style=\"display:none;\" />\n","  <br/>\n","  <button id=\"generate-btn\" disabled>Generate Caption</button>\n","  <div id=\"caption-output\"></div>\n","</div>\n","\n","<script>\n","  const input = document.getElementById('fileInput');\n","  const preview = document.getElementById('preview');\n","  const generateBtn = document.getElementById('generate-btn');\n","  const captionOutput = document.getElementById('caption-output');\n","\n","  input.onchange = evt => {\n","    const [file] = input.files;\n","    if (file) {\n","      preview.src = URL.createObjectURL(file);\n","      preview.style.display = 'block';\n","      generateBtn.disabled = false;\n","      captionOutput.textContent = '';\n","\n","      const reader = new FileReader();\n","      reader.onload = function() {\n","        const arrayBuffer = reader.result;\n","        const bytes = new Uint8Array(arrayBuffer);\n","        google.colab.kernel.invokeFunction('notebook.upload_image', [Array.from(bytes)], {});\n","      };\n","      reader.readAsArrayBuffer(file);\n","    }\n","  };\n","\n","  generateBtn.onclick = () => {\n","    captionOutput.textContent = 'Generating caption...';\n","\n","    google.colab.kernel.invokeFunction('notebook.generate_caption', [], {}).then(result => {\n","      captionOutput.textContent = result.data['text/plain'].replace(/'/g, \"\");\n","    }).catch(() => {\n","      captionOutput.textContent = '⚠️ Error generating caption';\n","    });\n","  };\n","</script>\n"]},"metadata":{}}]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","\n","display(HTML('''\n","<link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Poppins:wght@400;600&display=swap\" rel=\"stylesheet\">\n","\n","<style>\n","/* --- Base styles --- */\n","body {\n","  font-family: 'Poppins', sans-serif;\n","  background: #000;\n","  padding: 40px;\n","  text-align: center;\n","  color: #fff;\n","  min-height: 100vh;\n","  box-sizing: border-box;\n","}\n","\n","/* --- Responsive & Layout --- */\n",".glass-card {\n","  backdrop-filter: blur(25px) saturate(180%);\n","  -webkit-backdrop-filter: blur(25px) saturate(180%);\n","  background: rgba(255, 255, 255, 0.08);\n","  border-radius: 28px;\n","  border: 1px solid rgba(255, 255, 255, 0.15);\n","  padding: 32px 24px 28px 24px;\n","  max-width: 400px;\n","  margin: 40px auto;\n","  box-shadow: 0 4px 30px rgba(0,0,0,0.6);\n","  transition: transform 0.3s, box-shadow 0.3s;\n","  position: relative;\n","}\n","@media (max-width: 600px) {\n","  .glass-card {\n","    max-width: 96vw;\n","    padding: 16px 6vw 20px 6vw;\n","  }\n","}\n","\n","/* --- Step Progress Indicator --- */\n",".steps {\n","  display: flex;\n","  justify-content: space-between;\n","  margin-bottom: 20px;\n","  gap: 3px;\n","  font-size: 1rem;\n","}\n",".step {\n","  flex: 1 1 0;\n","  text-align: center;\n","  padding: 7px 0 5px 0;\n","  border-radius: 18px;\n","  background: rgba(255,255,255,0.05);\n","  color: #8eeeff;\n","  font-weight: 500;\n","  transition: background 0.3s, color 0.3s;\n","  letter-spacing: 0.01em;\n","  user-select: none;\n","}\n",".step.active {\n","  background: linear-gradient(90deg, #3c8ce7 60%, #00eaff 100%);\n","  color: #172a40;\n","  font-weight: 700;\n","  box-shadow: 0 2px 12px #00eaff40;\n","}\n","\n","/* --- Heading Gradient --- */\n","h2 {\n","  font-size: 2.1rem;\n","  font-weight: 600;\n","  margin-bottom: 14px;\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  -webkit-background-clip: text;\n","  -webkit-text-fill-color: transparent;\n","  letter-spacing: 0.02em;\n","}\n","\n","/* --- Drag & Drop --- */\n","#dropzone {\n","  border: 2px dashed #3c8ce7;\n","  border-radius: 18px;\n","  background: rgba(50, 95, 170, 0.05);\n","  color: #8eeeff;\n","  padding: 28px 0 18px 0;\n","  margin-bottom: 12px;\n","  font-size: 1.04rem;\n","  transition: border-color 0.3s, background 0.3s;\n","  cursor: pointer;\n","  position: relative;\n","}\n","#dropzone.dragover {\n","  border-color: #00eaff;\n","  background: rgba(0,234,255,0.07);\n","}\n","#dropzone:focus {\n","  outline: 2px solid #00eaff;\n","}\n","\n","/* --- File input / Demo button --- */\n","#fileInput, #demo-btn {\n","  margin: 6px 0 0 0;\n","  padding: 12px 22px;\n","  font-weight: 500;\n","  font-size: 1rem;\n","  border: none;\n","  border-radius: 18px;\n","  background: rgba(255,255,255,0.14);\n","  color: #fff;\n","  backdrop-filter: blur(12px);\n","  cursor: pointer;\n","  transition: background 0.3s, transform 0.1s;\n","}\n","#fileInput:hover, #demo-btn:hover {\n","  background: rgba(255,255,255,0.26);\n","  transform: scale(1.03);\n","}\n","#fileInput:focus, #demo-btn:focus {\n","  outline: 2px solid #00eaff;\n","}\n","/* Demo Button special */\n","#demo-btn {\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  color: #192e3a;\n","  margin-left: 7px;\n","  font-weight: 600;\n","}\n","\n","/* --- Image preview --- */\n","#preview {\n","  width: 100%;\n","  border-radius: 18px;\n","  box-shadow: 0 4px 18px rgba(0,0,0,0.5);\n","  margin: 18px 0 12px 0;\n","  display: none;\n","  outline: 0;\n","  transition: box-shadow 0.2s;\n","}\n","#preview:focus {\n","  box-shadow: 0 0 0 3px #00eaff80;\n","}\n","\n","/* --- Generate Button --- */\n","#generate-btn {\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  border: none;\n","  padding: 12px 28px;\n","  font-size: 1.02rem;\n","  font-weight: 700;\n","  border-radius: 28px;\n","  cursor: pointer;\n","  color: #fff;\n","  box-shadow: 0 5px 20px rgba(0,122,255,0.3);\n","  margin: 13px auto 4px auto;\n","  transition: transform 0.2s, box-shadow 0.3s, background 0.2s;\n","  display: block;\n","}\n","#generate-btn:disabled {\n","  background: rgba(255,255,255,0.12);\n","  color: #b2b2b2;\n","  cursor: not-allowed;\n","  box-shadow: none;\n","}\n","#generate-btn:focus {\n","  outline: 2px solid #00eaff;\n","}\n","#generate-btn:hover:not(:disabled) {\n","  transform: translateY(-2px) scale(1.03);\n","  box-shadow: 0 9px 36px #00eaff55;\n","}\n","\n","/* --- Spinner loader --- */\n","#spinner {\n","  display: none;\n","  margin: 15px auto 2px auto;\n","  width: 37px;\n","  height: 37px;\n","}\n",".spinner-inner {\n","  box-sizing: border-box;\n","  border: 4px solid #3c8ce7;\n","  border-top: 4px solid #00eaff;\n","  border-radius: 50%;\n","  width: 37px;\n","  height: 37px;\n","  animation: spin 0.8s linear infinite;\n","  margin: auto;\n","}\n","@keyframes spin {\n","  0% { transform: rotate(0);}\n","  100% { transform: rotate(360deg);}\n","}\n","\n","/* --- Caption Output --- */\n","#caption-output {\n","  min-height: 38px;\n","  margin-top: 16px;\n","  font-size: 1.09rem;\n","  font-family: 'Libre Baskerville', serif;\n","  color: #fff;\n","  background: rgba(60, 140, 231, 0.08);\n","  border-radius: 12px;\n","  padding: 10px;\n","  box-shadow: 0 2px 12px #3c8ce715;\n","  transition: background 0.2s;\n","  word-break: break-word;\n","}\n","\n","/* --- Error state --- */\n","#error-msg {\n","  color: #ff6e6e;\n","  font-weight: 600;\n","  margin: 7px 0 2px 0;\n","  display: none;\n","}\n","\n","/* --- Accessibility: focus and ARIA enhancements, improved contrast, transitions --- */\n","/* --- Micro-interaction: soft glows, animated step highlight, hover/focus as above --- */\n","</style>\n","\n","<div class=\"glass-card\" role=\"region\" aria-label=\"Image Caption Generator Card\">\n","  <h2 tabindex=\"0\">Automatic Image Caption Generator</h2>\n","\n","  <div class=\"steps\" aria-label=\"Steps Indicator\">\n","\n","    <div class=\"step\" id=\"step1\">1. Upload</div>\n","    <div class=\"step\" id=\"step2\">2. Preview</div>\n","    <div class=\"step\" id=\"step3\">3. Generate</div>\n","  </div>\n","  <div id=\"dropzone\" tabindex=\"0\" aria-label=\"File drop area or click to select image\">\n","    <span id=\"dropzonetext\">Drag & drop an image here, or click to select</span>\n","    <input type=\"file\" id=\"fileInput\" accept=\"image/*\" aria-label=\"Choose image file\" style=\"opacity:0;width:0.1px;position:absolute;z-index:-1;\" />\n","    <button id=\"demo-btn\" type=\"button\" aria-label=\"Try sample image\">Demo Image</button>\n","  </div>\n","  <img id=\"preview\" src=\"\" alt=\"Selected preview\" tabindex=\"0\" />\n","  <span id=\"error-msg\" role=\"alert\"></span>\n","  <button id=\"generate-btn\" disabled aria-label=\"Generate Caption\">Generate Caption</button>\n","  <div id=\"spinner\"><div class=\"spinner-inner\" aria-hidden=\"true\"></div></div>\n","  <div id=\"caption-output\" aria-live=\"polite\"></div>\n","</div>\n","\n","<script>\n","// --- ELEMENTS ---\n","const steps = [document.getElementById('step1'), document.getElementById('step2'), document.getElementById('step3')];\n","const dropzone = document.getElementById('dropzone');\n","const fileInput = document.getElementById('fileInput');\n","const preview = document.getElementById('preview');\n","const generateBtn = document.getElementById('generate-btn');\n","const captionOutput = document.getElementById('caption-output');\n","const errorMsg = document.getElementById('error-msg');\n","const spinner = document.getElementById('spinner');\n","const demoBtn = document.getElementById('demo-btn');\n","const dropzonetext = document.getElementById('dropzonetext');\n","\n","// SVG demo image (a colorful placeholder, replace with a real img if desired)\n","const demoImgURL = 'https://upload.wikimedia.org/wikipedia/commons/8/89/Portrait_Placeholder.png';\n","\n","// --- ACCESSIBILITY helpers ---\n","dropzone.addEventListener('keydown', function(e) {\n","  if (e.key === ' ' || e.key === 'Enter') {\n","    fileInput.click();\n","    e.preventDefault();\n","  }\n","});\n","dropzone.addEventListener('click', function() {\n","  fileInput.click();\n","});\n","\n","// --- DRAG & DROP ---\n","dropzone.addEventListener('dragover', (e) => {\n","  e.preventDefault(); dropzone.classList.add('dragover');\n","});\n","dropzone.addEventListener('dragleave', (e) => {dropzone.classList.remove('dragover');});\n","dropzone.addEventListener('drop', (e) => {\n","  e.preventDefault(); dropzone.classList.remove('dragover');\n","  if (e.dataTransfer.files.length > 0) {\n","    fileInput.files = e.dataTransfer.files;\n","    fileInput.dispatchEvent(new Event('change'));\n","  }\n","});\n","\n","// --- INPUT HANDLERS ---\n","fileInput.onchange = evt => {\n","  errorMsg.style.display = \"none\";\n","  const [file] = fileInput.files;\n","  if (!file) return;\n","  if (!file.type.startsWith('image/')) {\n","    errorMsg.textContent = \"File is not an image!\"; errorMsg.style.display = \"block\";\n","    return;\n","  }\n","  preview.src = URL.createObjectURL(file);\n","  preview.style.display = 'block';\n","  preview.setAttribute('aria-label', 'Selected image preview');\n","  // Update steps\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[1].classList.add('active');\n","  generateBtn.disabled = false; captionOutput.textContent = '';\n","  // Upload to kernel for notebook\n","  const reader = new FileReader();\n","  reader.onload = function() {\n","    const arrayBuffer = reader.result;\n","    const bytes = new Uint8Array(arrayBuffer);\n","    if (window.google && google.colab && google.colab.kernel)\n","      google.colab.kernel.invokeFunction('notebook.upload_image', [Array.from(bytes)], {});\n","  };\n","  reader.readAsArrayBuffer(file);\n","};\n","\n","// --- DEMO BUTTON HANDLER ---\n","demoBtn.onclick = () => {\n","  errorMsg.style.display = \"none\";\n","  preview.src = demoImgURL;\n","  preview.style.display = 'block';\n","  preview.setAttribute('aria-label', 'Sample image preview');\n","  // Simulate step\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[1].classList.add('active');\n","  generateBtn.disabled = false; captionOutput.textContent = '';\n","  // If desired, also load into kernel via fetch/XHR...\n","};\n","\n","// --- GENERATE CAPTION ---\n","generateBtn.onclick = () => {\n","  errorMsg.style.display = \"none\";\n","  spinner.style.display = \"block\";\n","  captionOutput.textContent = '';\n","  generateBtn.disabled = true;\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[2].classList.add('active');\n","\n","  // Always open new tab on user interaction for popup unblock\n","  let outputTab = window.open('', '_blank');\n","  if (!outputTab) {\n","    captionOutput.textContent = \"⚠️ Please allow pop-ups for this site to view the output in a new tab.\";\n","    spinner.style.display = \"none\";\n","    generateBtn.disabled = false;\n","    return;\n","  }\n","\n","  // Helper to build and update the tab\n","  function updateTabUI(imgSrc, captionText) {\n","    let tabHtml = `\n","    <html>\n","    <head>\n","      <title>Automatic Image Caption Generator Output</title>\n","      <link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Poppins:wght@400;600&display=swap\" rel=\"stylesheet\">\n","      <style>\n","        body {font-family: 'Poppins', sans-serif; background: #000; color: #fff; padding: 40px 0; margin: 0;}\n","        .glass-card {\n","          backdrop-filter: blur(25px) saturate(180%);\n","          -webkit-backdrop-filter: blur(25px) saturate(180%);\n","          background: rgba(255,255,255,0.08); border-radius: 28px;\n","          border: 1px solid rgba(255,255,255,0.15); padding: 38px 22px 32px 22px;\n","          max-width: 400px; margin: 48px auto;\n","          box-shadow: 0 4px 30px rgba(0,0,0,0.6); text-align: center;\n","        }\n","        h2 {\n","          font-size: 2.1rem;font-weight: 600;margin-bottom: 14px;\n","          background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","          -webkit-background-clip: text;-webkit-text-fill-color: transparent;\n","        }\n","        .steps {\n","          display: flex;justify-content: space-between;margin-bottom: 18px;gap:2.5px;font-size: 1rem;\n","        }\n","        .step {\n","          flex: 1 1 0;text-align: center;padding:6px 0 4px 0;border-radius:18px;\n","          background: rgba(255,255,255,0.05); color:#8eeeff;font-weight:500;\n","        }\n","        .step.active { background: linear-gradient(90deg,#3c8ce7 60%,#00eaff 100%);\n","          color:#172a40;font-weight:700;box-shadow:0 2px 12px #00eaff40;}\n","        #preview { width: 100%; border-radius: 18px; box-shadow: 0 4px 18px rgba(0,0,0,0.5); margin:15px 0 12px 0;}\n","        #caption-output {\n","          min-height:38px;margin-top:12px;\n","          font-size:1.09rem;font-family:'Libre Baskerville', serif;\n","          color:#fff;background:rgba(60,140,231,0.08);\n","          border-radius:12px;padding:10px;box-shadow:0 2px 12px #3c8ce715;\n","          transition: background 0.2s;word-break:break-word;\n","        }\n","      </style>\n","    </head>\n","    <body>\n","      <div class=\"glass-card\" role=\"region\" aria-label=\"Image Caption Generator Card\">\n","        <h2 tabindex=\"0\">Automatic Image Caption Generator</h2>\n","        <div class=\"steps\" aria-label=\"Steps Indicator\">\n","          <div class=\"step\">1. Upload</div>\n","          <div class=\"step\">2. Preview</div>\n","          <div class=\"step active\">3. Generate</div>\n","        </div>\n","        <img id=\"preview\" src=\"${imgSrc}\" alt=\"Selected preview\" tabindex=\"0\" style=\"display:block;\" />\n","        <div id=\"caption-output\" aria-live=\"polite\">${captionText ? captionText : 'Generating caption...'}</div>\n","      </div>\n","    </body>\n","    </html>\n","    `;\n","    outputTab.document.open(); outputTab.document.write(tabHtml); outputTab.document.close();\n","  }\n","\n","  // Get image for output tab: uploaded (file) or demo (URL)\n","  let imgSrc = preview.src;\n","  let isDemo = !(imgSrc.startsWith('blob:'));\n","  // Show initial UI in output tab (loading spinner equivalent)\n","  updateTabUI(imgSrc, \"Generating caption...\");\n","\n","  // Now run the caption kernel function as before\n","  if (window.google && google.colab && google.colab.kernel) {\n","    google.colab.kernel.invokeFunction('notebook.generate_caption', [], {})\n","      .then(result => {\n","        spinner.style.display = \"none\";\n","        let text = (result && result.data && result.data['text/plain']) ? result.data['text/plain'].replace(/'/g, \"\") : \"\";\n","        // For uploaded file, re-convert to img DataURL so it always works cross-tab\n","        if (!isDemo) {\n","          // Find the file from file input\n","          let file = fileInput.files[0];\n","          if (file) {\n","            let reader = new FileReader();\n","            reader.onload = function(evt) {\n","              let dataUrl = evt.target.result;\n","              updateTabUI(dataUrl, text || \"No caption returned.\");\n","            }\n","            reader.readAsDataURL(file);\n","          } else {\n","            updateTabUI('', text || \"No caption returned.\");\n","          }\n","        } else {\n","          updateTabUI(imgSrc, text || \"No caption returned.\");\n","        }\n","        generateBtn.disabled = false;\n","      })\n","      .catch(() => {\n","        spinner.style.display = \"none\";\n","        let errMsg = \"⚠️ Error generating caption\";\n","        if (!isDemo) {\n","          let file = fileInput.files;\n","          if (file) {\n","            let reader = new FileReader();\n","            reader.onload = function(evt) {\n","              updateTabUI(evt.target.result, errMsg);\n","            }\n","            reader.readAsDataURL(file);\n","          } else {\n","            updateTabUI('', errMsg);\n","          }\n","        } else {\n","          updateTabUI(imgSrc, errMsg);\n","        }\n","        generateBtn.disabled = false;\n","      });\n","  } else {\n","    setTimeout(() => {\n","      spinner.style.display = \"none\";\n","      let errMsg = \"Notebook kernel API not available.\";\n","      if (!isDemo) {\n","        let file = fileInput.files;\n","        if (file) {\n","          let reader = new FileReader();\n","          reader.onload = function(evt) {\n","            updateTabUI(evt.target.result, errMsg);\n","          }\n","          reader.readAsDataURL(file);\n","        } else {\n","          updateTabUI('', errMsg);\n","        }\n","      } else {\n","        updateTabUI(imgSrc, errMsg);\n","      }\n","      generateBtn.disabled = false;\n","    }, 800);\n","  }\n","};\n","\n","\n","\n","// --- INITIAL STATE STEP HIGHLIGHT ---\n","steps.forEach(s=>s.classList.remove('active'));\n","steps[0].classList.add('active');\n","\n","/* --- Keyboard a11y for demo button, preview, etc. */\n","demoBtn.tabIndex = 0;\n","preview.tabIndex = 0;\n","fileInput.tabIndex = -1;\n","\n","/* --- Error state reset for every action --- */\n","['click','focus','input'].forEach(ev=>{\n","  preview.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","  fileInput.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","  demoBtn.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","});\n","</script>\n","'''))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"etJHeXL43yaf","executionInfo":{"status":"ok","timestamp":1755672197550,"user_tz":-330,"elapsed":155,"user":{"displayName":"22H51A6726-KODAM DEEPSHIK B.Tech-CSD(2022-26)","userId":"00991348723686241262"}},"outputId":"9ee0ca0e-2c73-4374-94b9-156688483a95"},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Poppins:wght@400;600&display=swap\" rel=\"stylesheet\">\n","\n","<style>\n","/* --- Base styles --- */\n","body {\n","  font-family: 'Poppins', sans-serif;\n","  background: #000;\n","  padding: 40px;\n","  text-align: center;\n","  color: #fff;\n","  min-height: 100vh;\n","  box-sizing: border-box;\n","}\n","\n","/* --- Responsive & Layout --- */\n",".glass-card {\n","  backdrop-filter: blur(25px) saturate(180%);\n","  -webkit-backdrop-filter: blur(25px) saturate(180%);\n","  background: rgba(255, 255, 255, 0.08);\n","  border-radius: 28px;\n","  border: 1px solid rgba(255, 255, 255, 0.15);\n","  padding: 32px 24px 28px 24px;\n","  max-width: 400px;\n","  margin: 40px auto;\n","  box-shadow: 0 4px 30px rgba(0,0,0,0.6);\n","  transition: transform 0.3s, box-shadow 0.3s;\n","  position: relative;\n","}\n","@media (max-width: 600px) {\n","  .glass-card {\n","    max-width: 96vw;\n","    padding: 16px 6vw 20px 6vw;\n","  }\n","}\n","\n","/* --- Step Progress Indicator --- */\n",".steps {\n","  display: flex;\n","  justify-content: space-between;\n","  margin-bottom: 20px;\n","  gap: 3px;\n","  font-size: 1rem;\n","}\n",".step {\n","  flex: 1 1 0;\n","  text-align: center;\n","  padding: 7px 0 5px 0;\n","  border-radius: 18px;\n","  background: rgba(255,255,255,0.05);\n","  color: #8eeeff;\n","  font-weight: 500;\n","  transition: background 0.3s, color 0.3s;\n","  letter-spacing: 0.01em;\n","  user-select: none;\n","}\n",".step.active {\n","  background: linear-gradient(90deg, #3c8ce7 60%, #00eaff 100%);\n","  color: #172a40;\n","  font-weight: 700;\n","  box-shadow: 0 2px 12px #00eaff40;\n","}\n","\n","/* --- Heading Gradient --- */\n","h2 {\n","  font-size: 2.1rem;\n","  font-weight: 600;\n","  margin-bottom: 14px;\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  -webkit-background-clip: text;\n","  -webkit-text-fill-color: transparent;\n","  letter-spacing: 0.02em;\n","}\n","\n","/* --- Drag & Drop --- */\n","#dropzone {\n","  border: 2px dashed #3c8ce7;\n","  border-radius: 18px;\n","  background: rgba(50, 95, 170, 0.05);\n","  color: #8eeeff;\n","  padding: 28px 0 18px 0;\n","  margin-bottom: 12px;\n","  font-size: 1.04rem;\n","  transition: border-color 0.3s, background 0.3s;\n","  cursor: pointer;\n","  position: relative;\n","}\n","#dropzone.dragover {\n","  border-color: #00eaff;\n","  background: rgba(0,234,255,0.07);\n","}\n","#dropzone:focus {\n","  outline: 2px solid #00eaff;\n","}\n","\n","/* --- File input / Demo button --- */\n","#fileInput, #demo-btn {\n","  margin: 6px 0 0 0;\n","  padding: 12px 22px;\n","  font-weight: 500;\n","  font-size: 1rem;\n","  border: none;\n","  border-radius: 18px;\n","  background: rgba(255,255,255,0.14);\n","  color: #fff;\n","  backdrop-filter: blur(12px);\n","  cursor: pointer;\n","  transition: background 0.3s, transform 0.1s;\n","}\n","#fileInput:hover, #demo-btn:hover {\n","  background: rgba(255,255,255,0.26);\n","  transform: scale(1.03);\n","}\n","#fileInput:focus, #demo-btn:focus {\n","  outline: 2px solid #00eaff;\n","}\n","/* Demo Button special */\n","#demo-btn {\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  color: #192e3a;\n","  margin-left: 7px;\n","  font-weight: 600;\n","}\n","\n","/* --- Image preview --- */\n","#preview {\n","  width: 100%;\n","  border-radius: 18px;\n","  box-shadow: 0 4px 18px rgba(0,0,0,0.5);\n","  margin: 18px 0 12px 0;\n","  display: none;\n","  outline: 0;\n","  transition: box-shadow 0.2s;\n","}\n","#preview:focus {\n","  box-shadow: 0 0 0 3px #00eaff80;\n","}\n","\n","/* --- Generate Button --- */\n","#generate-btn {\n","  background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","  border: none;\n","  padding: 12px 28px;\n","  font-size: 1.02rem;\n","  font-weight: 700;\n","  border-radius: 28px;\n","  cursor: pointer;\n","  color: #fff;\n","  box-shadow: 0 5px 20px rgba(0,122,255,0.3);\n","  margin: 13px auto 4px auto;\n","  transition: transform 0.2s, box-shadow 0.3s, background 0.2s;\n","  display: block;\n","}\n","#generate-btn:disabled {\n","  background: rgba(255,255,255,0.12);\n","  color: #b2b2b2;\n","  cursor: not-allowed;\n","  box-shadow: none;\n","}\n","#generate-btn:focus {\n","  outline: 2px solid #00eaff;\n","}\n","#generate-btn:hover:not(:disabled) {\n","  transform: translateY(-2px) scale(1.03);\n","  box-shadow: 0 9px 36px #00eaff55;\n","}\n","\n","/* --- Spinner loader --- */\n","#spinner {\n","  display: none;\n","  margin: 15px auto 2px auto;\n","  width: 37px;\n","  height: 37px;\n","}\n",".spinner-inner {\n","  box-sizing: border-box;\n","  border: 4px solid #3c8ce7;\n","  border-top: 4px solid #00eaff;\n","  border-radius: 50%;\n","  width: 37px;\n","  height: 37px;\n","  animation: spin 0.8s linear infinite;\n","  margin: auto;\n","}\n","@keyframes spin {\n","  0% { transform: rotate(0);}\n","  100% { transform: rotate(360deg);}\n","}\n","\n","/* --- Caption Output --- */\n","#caption-output {\n","  min-height: 38px;\n","  margin-top: 16px;\n","  font-size: 1.09rem;\n","  font-family: 'Libre Baskerville', serif;\n","  color: #fff;\n","  background: rgba(60, 140, 231, 0.08);\n","  border-radius: 12px;\n","  padding: 10px;\n","  box-shadow: 0 2px 12px #3c8ce715;\n","  transition: background 0.2s;\n","  word-break: break-word;\n","}\n","\n","/* --- Error state --- */\n","#error-msg {\n","  color: #ff6e6e;\n","  font-weight: 600;\n","  margin: 7px 0 2px 0;\n","  display: none;\n","}\n","\n","/* --- Accessibility: focus and ARIA enhancements, improved contrast, transitions --- */\n","/* --- Micro-interaction: soft glows, animated step highlight, hover/focus as above --- */\n","</style>\n","\n","<div class=\"glass-card\" role=\"region\" aria-label=\"Image Caption Generator Card\">\n","  <h2 tabindex=\"0\">Automatic Image Caption Generator</h2>\n","\n","  <div class=\"steps\" aria-label=\"Steps Indicator\">\n","\n","    <div class=\"step\" id=\"step1\">1. Upload</div>\n","    <div class=\"step\" id=\"step2\">2. Preview</div>\n","    <div class=\"step\" id=\"step3\">3. Generate</div>\n","  </div>\n","  <div id=\"dropzone\" tabindex=\"0\" aria-label=\"File drop area or click to select image\">\n","    <span id=\"dropzonetext\">Drag & drop an image here, or click to select</span>\n","    <input type=\"file\" id=\"fileInput\" accept=\"image/*\" aria-label=\"Choose image file\" style=\"opacity:0;width:0.1px;position:absolute;z-index:-1;\" />\n","    <button id=\"demo-btn\" type=\"button\" aria-label=\"Try sample image\">Demo Image</button>\n","  </div>\n","  <img id=\"preview\" src=\"\" alt=\"Selected preview\" tabindex=\"0\" />\n","  <span id=\"error-msg\" role=\"alert\"></span>\n","  <button id=\"generate-btn\" disabled aria-label=\"Generate Caption\">Generate Caption</button>\n","  <div id=\"spinner\"><div class=\"spinner-inner\" aria-hidden=\"true\"></div></div>\n","  <div id=\"caption-output\" aria-live=\"polite\"></div>\n","</div>\n","\n","<script>\n","// --- ELEMENTS ---\n","const steps = [document.getElementById('step1'), document.getElementById('step2'), document.getElementById('step3')];\n","const dropzone = document.getElementById('dropzone');\n","const fileInput = document.getElementById('fileInput');\n","const preview = document.getElementById('preview');\n","const generateBtn = document.getElementById('generate-btn');\n","const captionOutput = document.getElementById('caption-output');\n","const errorMsg = document.getElementById('error-msg');\n","const spinner = document.getElementById('spinner');\n","const demoBtn = document.getElementById('demo-btn');\n","const dropzonetext = document.getElementById('dropzonetext');\n","\n","// SVG demo image (a colorful placeholder, replace with a real img if desired)\n","const demoImgURL = 'https://upload.wikimedia.org/wikipedia/commons/8/89/Portrait_Placeholder.png';\n","\n","// --- ACCESSIBILITY helpers ---\n","dropzone.addEventListener('keydown', function(e) {\n","  if (e.key === ' ' || e.key === 'Enter') {\n","    fileInput.click();\n","    e.preventDefault();\n","  }\n","});\n","dropzone.addEventListener('click', function() {\n","  fileInput.click();\n","});\n","\n","// --- DRAG & DROP ---\n","dropzone.addEventListener('dragover', (e) => {\n","  e.preventDefault(); dropzone.classList.add('dragover');\n","});\n","dropzone.addEventListener('dragleave', (e) => {dropzone.classList.remove('dragover');});\n","dropzone.addEventListener('drop', (e) => {\n","  e.preventDefault(); dropzone.classList.remove('dragover');\n","  if (e.dataTransfer.files.length > 0) {\n","    fileInput.files = e.dataTransfer.files;\n","    fileInput.dispatchEvent(new Event('change'));\n","  }\n","});\n","\n","// --- INPUT HANDLERS ---\n","fileInput.onchange = evt => {\n","  errorMsg.style.display = \"none\";\n","  const [file] = fileInput.files;\n","  if (!file) return;\n","  if (!file.type.startsWith('image/')) {\n","    errorMsg.textContent = \"File is not an image!\"; errorMsg.style.display = \"block\";\n","    return;\n","  }\n","  preview.src = URL.createObjectURL(file);\n","  preview.style.display = 'block';\n","  preview.setAttribute('aria-label', 'Selected image preview');\n","  // Update steps\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[1].classList.add('active');\n","  generateBtn.disabled = false; captionOutput.textContent = '';\n","  // Upload to kernel for notebook\n","  const reader = new FileReader();\n","  reader.onload = function() {\n","    const arrayBuffer = reader.result;\n","    const bytes = new Uint8Array(arrayBuffer);\n","    if (window.google && google.colab && google.colab.kernel)\n","      google.colab.kernel.invokeFunction('notebook.upload_image', [Array.from(bytes)], {});\n","  };\n","  reader.readAsArrayBuffer(file);\n","};\n","\n","// --- DEMO BUTTON HANDLER ---\n","demoBtn.onclick = () => {\n","  errorMsg.style.display = \"none\";\n","  preview.src = demoImgURL;\n","  preview.style.display = 'block';\n","  preview.setAttribute('aria-label', 'Sample image preview');\n","  // Simulate step\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[1].classList.add('active');\n","  generateBtn.disabled = false; captionOutput.textContent = '';\n","  // If desired, also load into kernel via fetch/XHR...\n","};\n","\n","// --- GENERATE CAPTION ---\n","generateBtn.onclick = () => {\n","  errorMsg.style.display = \"none\";\n","  spinner.style.display = \"block\";\n","  captionOutput.textContent = '';\n","  generateBtn.disabled = true;\n","  steps.forEach(s=>s.classList.remove('active'));\n","  steps[2].classList.add('active');\n","\n","  // Always open new tab on user interaction for popup unblock\n","  let outputTab = window.open('', '_blank');\n","  if (!outputTab) {\n","    captionOutput.textContent = \"⚠️ Please allow pop-ups for this site to view the output in a new tab.\";\n","    spinner.style.display = \"none\";\n","    generateBtn.disabled = false;\n","    return;\n","  }\n","\n","  // Helper to build and update the tab\n","  function updateTabUI(imgSrc, captionText) {\n","    let tabHtml = `\n","    <html>\n","    <head>\n","      <title>Automatic Image Caption Generator Output</title>\n","      <link href=\"https://fonts.googleapis.com/css2?family=Libre+Baskerville&family=Poppins:wght@400;600&display=swap\" rel=\"stylesheet\">\n","      <style>\n","        body {font-family: 'Poppins', sans-serif; background: #000; color: #fff; padding: 40px 0; margin: 0;}\n","        .glass-card {\n","          backdrop-filter: blur(25px) saturate(180%);\n","          -webkit-backdrop-filter: blur(25px) saturate(180%);\n","          background: rgba(255,255,255,0.08); border-radius: 28px;\n","          border: 1px solid rgba(255,255,255,0.15); padding: 38px 22px 32px 22px;\n","          max-width: 400px; margin: 48px auto;\n","          box-shadow: 0 4px 30px rgba(0,0,0,0.6); text-align: center;\n","        }\n","        h2 {\n","          font-size: 2.1rem;font-weight: 600;margin-bottom: 14px;\n","          background: linear-gradient(90deg, #3c8ce7, #00eaff);\n","          -webkit-background-clip: text;-webkit-text-fill-color: transparent;\n","        }\n","        .steps {\n","          display: flex;justify-content: space-between;margin-bottom: 18px;gap:2.5px;font-size: 1rem;\n","        }\n","        .step {\n","          flex: 1 1 0;text-align: center;padding:6px 0 4px 0;border-radius:18px;\n","          background: rgba(255,255,255,0.05); color:#8eeeff;font-weight:500;\n","        }\n","        .step.active { background: linear-gradient(90deg,#3c8ce7 60%,#00eaff 100%);\n","          color:#172a40;font-weight:700;box-shadow:0 2px 12px #00eaff40;}\n","        #preview { width: 100%; border-radius: 18px; box-shadow: 0 4px 18px rgba(0,0,0,0.5); margin:15px 0 12px 0;}\n","        #caption-output {\n","          min-height:38px;margin-top:12px;\n","          font-size:1.09rem;font-family:'Libre Baskerville', serif;\n","          color:#fff;background:rgba(60,140,231,0.08);\n","          border-radius:12px;padding:10px;box-shadow:0 2px 12px #3c8ce715;\n","          transition: background 0.2s;word-break:break-word;\n","        }\n","      </style>\n","    </head>\n","    <body>\n","      <div class=\"glass-card\" role=\"region\" aria-label=\"Image Caption Generator Card\">\n","        <h2 tabindex=\"0\">Automatic Image Caption Generator</h2>\n","        <div class=\"steps\" aria-label=\"Steps Indicator\">\n","          <div class=\"step\">1. Upload</div>\n","          <div class=\"step\">2. Preview</div>\n","          <div class=\"step active\">3. Generate</div>\n","        </div>\n","        <img id=\"preview\" src=\"${imgSrc}\" alt=\"Selected preview\" tabindex=\"0\" style=\"display:block;\" />\n","        <div id=\"caption-output\" aria-live=\"polite\">${captionText ? captionText : 'Generating caption...'}</div>\n","      </div>\n","    </body>\n","    </html>\n","    `;\n","    outputTab.document.open(); outputTab.document.write(tabHtml); outputTab.document.close();\n","  }\n","\n","  // Get image for output tab: uploaded (file) or demo (URL)\n","  let imgSrc = preview.src;\n","  let isDemo = !(imgSrc.startsWith('blob:'));\n","  // Show initial UI in output tab (loading spinner equivalent)\n","  updateTabUI(imgSrc, \"Generating caption...\");\n","\n","  // Now run the caption kernel function as before\n","  if (window.google && google.colab && google.colab.kernel) {\n","    google.colab.kernel.invokeFunction('notebook.generate_caption', [], {})\n","      .then(result => {\n","        spinner.style.display = \"none\";\n","        let text = (result && result.data && result.data['text/plain']) ? result.data['text/plain'].replace(/'/g, \"\") : \"\";\n","        // For uploaded file, re-convert to img DataURL so it always works cross-tab\n","        if (!isDemo) {\n","          // Find the file from file input\n","          let file = fileInput.files[0];\n","          if (file) {\n","            let reader = new FileReader();\n","            reader.onload = function(evt) {\n","              let dataUrl = evt.target.result;\n","              updateTabUI(dataUrl, text || \"No caption returned.\");\n","            }\n","            reader.readAsDataURL(file);\n","          } else {\n","            updateTabUI('', text || \"No caption returned.\");\n","          }\n","        } else {\n","          updateTabUI(imgSrc, text || \"No caption returned.\");\n","        }\n","        generateBtn.disabled = false;\n","      })\n","      .catch(() => {\n","        spinner.style.display = \"none\";\n","        let errMsg = \"⚠️ Error generating caption\";\n","        if (!isDemo) {\n","          let file = fileInput.files;\n","          if (file) {\n","            let reader = new FileReader();\n","            reader.onload = function(evt) {\n","              updateTabUI(evt.target.result, errMsg);\n","            }\n","            reader.readAsDataURL(file);\n","          } else {\n","            updateTabUI('', errMsg);\n","          }\n","        } else {\n","          updateTabUI(imgSrc, errMsg);\n","        }\n","        generateBtn.disabled = false;\n","      });\n","  } else {\n","    setTimeout(() => {\n","      spinner.style.display = \"none\";\n","      let errMsg = \"Notebook kernel API not available.\";\n","      if (!isDemo) {\n","        let file = fileInput.files;\n","        if (file) {\n","          let reader = new FileReader();\n","          reader.onload = function(evt) {\n","            updateTabUI(evt.target.result, errMsg);\n","          }\n","          reader.readAsDataURL(file);\n","        } else {\n","          updateTabUI('', errMsg);\n","        }\n","      } else {\n","        updateTabUI(imgSrc, errMsg);\n","      }\n","      generateBtn.disabled = false;\n","    }, 800);\n","  }\n","};\n","\n","\n","\n","// --- INITIAL STATE STEP HIGHLIGHT ---\n","steps.forEach(s=>s.classList.remove('active'));\n","steps[0].classList.add('active');\n","\n","/* --- Keyboard a11y for demo button, preview, etc. */\n","demoBtn.tabIndex = 0;\n","preview.tabIndex = 0;\n","fileInput.tabIndex = -1;\n","\n","/* --- Error state reset for every action --- */\n","['click','focus','input'].forEach(ev=>{\n","  preview.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","  fileInput.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","  demoBtn.addEventListener(ev, ()=>{errorMsg.style.display=\"none\"; });\n","});\n","</script>\n"]},"metadata":{}}]}]}